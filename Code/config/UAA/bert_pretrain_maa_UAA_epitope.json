{
    "name": "BERT-Epitope-Pretrain-UAA-MAA",
    "n_gpu": 1,

    "dataset": {
        "type": "MAADataset",
        "args":{
            "seq_dir": "../ProcessedData/merged/epitope.csv",
            "token_length_list": "2,3",
            "vocab_dir": "../ProcessedData/vocab/epitope-2-3.csv",
            "seed": 0,
            "seq_name": "epitope",
            "tokenizer_name": "UAA",
            "max_len": 64,
            "test_split": 0.01
        }
    },

    "model": {
        "bert": "bert",
        "args":{
            "hidden_size": 768,
            "num_hidden_layers": 12,
            "num_attention_heads": 12,
            "intermediate_size": 3072,
            "hidden_act": "gelu",
            "hidden_dropout_prob": 0.1,
            "attention_probs_dropout_prob": 0.1,
            "max_position_embeddings": 512,
            "type_vocab_size": 2,
            "initializer_range": 0.02,
            "position_embedding_type": "absolute"
        }
    },

    "metrics":{
        "blosum_dir": "../RawData/blosum62.json",
        "blosum": true
    },

    "trainer": {
        "epochs": 25,
        "batch_size": 512,
        "save_dir": "../Result/",
        "lr": 5e-5,
        "warmup": 0.1,
        "eval_accumulation_steps": 1,
        "logging_steps": 100
    }

}